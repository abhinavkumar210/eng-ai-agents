{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b38d715",
   "metadata": {},
   "source": [
    "# Assignment 2 Part 4\n",
    "\n",
    "Abhinav Kumar\n",
    "10/13/2025\n",
    "\n",
    "Feature Extraction + PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from anomalib.data import MVTec\n",
    "from anomalib.models import Patchcore\n",
    "from lightning import Trainer, seed_everything\n",
    "\n",
    "PG = dict(host=\"localhost\", port=5432, dbname=\"mvtec\", user=\"postgres\", password=\"postgres\", table=\"mvtec_embeddings\")\n",
    "CATEGORIES = (\"tile\", \"leather\", \"grid\")\n",
    "seed_everything(42)\n",
    "\n",
    "def get_conn():\n",
    "    return psycopg2.connect(host=PG[\"host\"], port=PG[\"port\"], dbname=PG[\"dbname\"], user=PG[\"user\"], password=PG[\"password\"])\n",
    "\n",
    "def ensure_table(dim: int):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE EXTENSION IF NOT EXISTS vector;\n",
    "    CREATE TABLE IF NOT EXISTS {PG['table']} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        category TEXT,\n",
    "        split TEXT,\n",
    "        image_path TEXT,\n",
    "        label TEXT,\n",
    "        anomaly_score DOUBLE PRECISION,\n",
    "        embedding VECTOR({dim}),\n",
    "        metadata JSONB\n",
    "    );\n",
    "    \"\"\"\n",
    "    with get_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(ddl)\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def global_feature(x: torch.Tensor):\n",
    "    if x.ndim == 4:\n",
    "        x = torch.nn.functional.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "    return x\n",
    "\n",
    "def extract_features(model, batch):\n",
    "    with torch.no_grad():\n",
    "        feats = model.model.backbone(batch[\"image\"])\n",
    "        if isinstance(feats, dict):\n",
    "            feats = list(feats.values())[-1]\n",
    "        vecs = global_feature(feats).cpu().numpy()\n",
    "        return vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a66b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_split_embeddings(model, dm, split: str, category: str):\n",
    "    loader = dm.train_dataloader() if split == \"train\" else dm.test_dataloader()\n",
    "    all_vecs, meta = [], []\n",
    "    for batch in loader:\n",
    "        vecs = extract_features(model, batch)\n",
    "        all_vecs.append(vecs)\n",
    "        B = vecs.shape[0]\n",
    "        paths = batch.get(\"image_path\", [\"\"]*B)\n",
    "        labels = batch.get(\"label\", [\"unknown\"]*B)\n",
    "        for i in range(B):\n",
    "            meta.append(dict(category=category, split=split, image_path=paths[i], label=str(labels[i]), anomaly_score=float(\"nan\")))\n",
    "    if not all_vecs:\n",
    "        return np.empty((0,0)), []\n",
    "    return np.concatenate(all_vecs, axis=0), meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert_embeddings(embeddings, meta_rows):\n",
    "    dim = embeddings.shape[1]\n",
    "    ensure_table(dim)\n",
    "    rows = []\n",
    "    for v, m in zip(embeddings, meta_rows):\n",
    "        rows.append((m[\"category\"], m[\"split\"], m[\"image_path\"], m[\"label\"], m[\"anomaly_score\"], list(map(float, v)), json.dumps(m)))\n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO {PG['table']}\n",
    "    (category, split, image_path, label, anomaly_score, embedding, metadata)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s::vector, %s);\n",
    "    \"\"\"\n",
    "    with get_conn() as conn, conn.cursor() as cur:\n",
    "        execute_batch(cur, sql, rows, page_size=256)\n",
    "        conn.commit()\n",
    "    print(f\"Inserted {len(rows)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cat in CATEGORIES:\n",
    "    print(f\"\\n[{cat}] building embeddings...\")\n",
    "    dm = MVTec(root=\"./data\", category=cat, image_size=256, task=\"segmentation\", download=True,\n",
    "               train_batch_size=8, eval_batch_size=8, num_workers=4)\n",
    "    model = Patchcore()\n",
    "    Trainer(accelerator=\"cpu\", devices=1, max_epochs=0, logger=False, enable_checkpointing=False).fit(model, dm)\n",
    "    tr_vecs, tr_meta = build_split_embeddings(model, dm, \"train\", cat)\n",
    "    if tr_vecs.size:\n",
    "        insert_embeddings(tr_vecs, tr_meta)\n",
    "    te_vecs, te_meta = build_split_embeddings(model, dm, \"test\", cat)\n",
    "    if te_vecs.size:\n",
    "        insert_embeddings(te_vecs, te_meta)\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
