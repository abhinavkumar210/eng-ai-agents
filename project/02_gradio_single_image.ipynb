{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bef63e3",
   "metadata": {},
   "source": [
    "# Project — Part 2: Gradio: Single Image\n",
    "\n",
    "Abhinav Kumar\n",
    "12/7/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83fead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import textwrap\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://host.docker.internal:11434\"\n",
    "MODEL_NAME = \"qwen2.5:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee5592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_page_image(pil_image: Image.Image) -> str:\n",
    "    \"\"\"Run Tesseract OCR on a full paper page screenshot (PIL image).\"\"\"\n",
    "    img = np.array(pil_image)\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    scale = 1.5\n",
    "    img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * scale), int(img.shape[0] * scale)),\n",
    "        interpolation=cv2.INTER_CUBIC,\n",
    "    )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, lang=\"eng\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ebb4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen test: 'OK'\n"
     ]
    }
   ],
   "source": [
    "def call_qwen(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call local Qwen via Ollama's /api/generate endpoint.\n",
    "    \"\"\"\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    System: {system_prompt}\n",
    "\n",
    "    User: {user_prompt}\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
    "    if not resp.ok:\n",
    "        print(\"Ollama error status:\", resp.status_code)\n",
    "        print(\"Ollama error body:\", resp.text[:500])\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    return data.get(\"response\", \"\").strip()\n",
    "\n",
    "test_answer = call_qwen(\n",
    "    \"You are a concise assistant.\",\n",
    "    \"Reply with the single word OK.\"\n",
    ")\n",
    "print(\"Qwen test:\", repr(test_answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "459fc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_table_image(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Run OCR on an image that primarily contains a table.\n",
    "    Uses Tesseract, similar to ocr_page_image.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"\"\n",
    "\n",
    "    img = np.array(image)\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Slight upscale\n",
    "    scale = 1.5\n",
    "    img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * scale), int(img.shape[0] * scale)),\n",
    "        interpolation=cv2.INTER_CUBIC,\n",
    "    )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, lang=\"eng\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def explain_table_from_image(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Given an image containing (mostly) a table, OCR it and ask Qwen\n",
    "    to explain what the table is telling us.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload an image that clearly shows the table.\"\n",
    "\n",
    "    table_text = ocr_table_image(image)\n",
    "    if not table_text.strip():\n",
    "        return \"I couldn't read any text from the table image. Try a clearer or closer crop.\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a student interpret tables in research papers. \"\n",
    "        \"You focus on trends, comparisons, and the main message of the table.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    Here is noisy OCR text from a table in a research paper. It may have imperfect alignment\n",
    "    and formatting. Cells are mostly separated by spaces, and rows by newlines.\n",
    "\n",
    "    ---- TABLE OCR TEXT ----\n",
    "    {table_text}\n",
    "    ---- END TABLE OCR TEXT ----\n",
    "\n",
    "    Task:\n",
    "    1. Reconstruct in your head what this table likely looks like (rows, columns).\n",
    "    2. Explain, in clear language, what this table is telling us:\n",
    "       - What quantities or metrics are being compared?\n",
    "       - Which rows/conditions/models perform best?\n",
    "       - What patterns or trade-offs are visible?\n",
    "    3. If you can infer it, say what the takeaway is in the context of a model or method comparison.\n",
    "\n",
    "    Be concise but specific: 2–4 short paragraphs max.\n",
    "    \"\"\")\n",
    "\n",
    "    answer = call_qwen(system_prompt, user_prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9417b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_detrimental_ablation_from_image(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Given an image containing an ablation table, OCR it and ask Qwen\n",
    "    to identify which ablation hurts performance the most and why.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload an image of the ablation table.\"\n",
    "\n",
    "    table_text = ocr_table_image(image)\n",
    "    if not table_text.strip():\n",
    "        return \"I couldn't read any text from the ablation table. Try a clearer or closer crop.\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI researcher analyzing ablation studies in computer vision papers. \"\n",
    "        \"You are careful and explicit about how you interpret the numbers.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    Below is noisy OCR text from an ablation study table in a research paper.\n",
    "    It may have imperfect alignment and formatting, but rows generally correspond\n",
    "    to different variants of a model, and columns to metrics.\n",
    "\n",
    "    ---- ABLATION TABLE OCR TEXT ----\n",
    "    {table_text}\n",
    "    ---- END ABLATION TABLE OCR TEXT ----\n",
    "\n",
    "    Task:\n",
    "    1. Reconstruct in your head which row is the 'full model' or baseline\n",
    "       (the configuration with all components turned on, if possible).\n",
    "    2. For each ablated variant (rows where one or more components are removed\n",
    "       or changed), look at the main performance metric (e.g., mIoU, mAP, PSNR).\n",
    "    3. Identify which ablation (i.e., which removed/changed component) causes\n",
    "       the largest drop in that main metric compared to the full model.\n",
    "    4. Explain in clear language:\n",
    "       - Which ablation is the most detrimental (name the component / setting).\n",
    "       - How large the performance drop is (roughly, from X to Y).\n",
    "       - Why this suggests that component is especially important for the method.\n",
    "\n",
    "    If there is ambiguity (e.g., multiple metrics or no clear baseline), explain\n",
    "    how you resolved it and what reasonable assumption you made.\n",
    "\n",
    "    Keep the answer to 2–4 short paragraphs.\n",
    "    \"\"\")\n",
    "\n",
    "    answer = call_qwen(system_prompt, user_prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b54277a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split OCR text into coarse 'paragraphs' based on blank lines.\n",
    "    Filters out very short fragments.\n",
    "    \"\"\"\n",
    "    raw_paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    paragraphs = []\n",
    "    for p in raw_paragraphs:\n",
    "        clean = p.strip()\n",
    "\n",
    "        if len(clean) >= 60:\n",
    "            paragraphs.append(clean)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def heuristic_score(paragraph: str) -> float:\n",
    "    \"\"\"\n",
    "    Very simple heuristic to guess importance:\n",
    "    - Gives points for key section-like words.\n",
    "    - Bonus for mentions of 'we propose', 'our method', etc.\n",
    "    \"\"\"\n",
    "    p = paragraph.lower()\n",
    "    score = 0.0\n",
    "\n",
    "    keywords = {\n",
    "        \"abstract\": 3.0,\n",
    "        \"introduction\": 3.0,\n",
    "        \"method\": 2.0,\n",
    "        \"approach\": 2.0,\n",
    "        \"efficientvit\": 2.0,\n",
    "        \"attention\": 1.5,\n",
    "        \"multi-scale\": 1.5,\n",
    "        \"dense prediction\": 1.5,\n",
    "        \"segmentation\": 1.0,\n",
    "        \"experiment\": 1.0,\n",
    "        \"results\": 1.0,\n",
    "        \"conclusion\": 2.0,\n",
    "    }\n",
    "\n",
    "    for kw, w in keywords.items():\n",
    "        if kw in p:\n",
    "            score += w\n",
    "\n",
    "    contribution_phrases = [\n",
    "        \"we propose\",\n",
    "        \"we present\",\n",
    "        \"in this paper\",\n",
    "        \"our method\",\n",
    "        \"our approach\",\n",
    "        \"we introduce\",\n",
    "    ]\n",
    "    for phrase in contribution_phrases:\n",
    "        if phrase in p:\n",
    "            score += 3.0\n",
    "\n",
    "    score += len(paragraph) / 100.0\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73bee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_highlighted_text_from_image(image: Image.Image, highlighted_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a page image and a user-provided highlighted passage (as text),\n",
    "    run OCR for context and ask Qwen to explain that passage in a tutorial style.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload a page image first.\"\n",
    "\n",
    "    if not highlighted_text or not highlighted_text.strip():\n",
    "        return \"Please paste or type the highlighted text you want explained.\"\n",
    "\n",
    "    page_text = ocr_page_image(image)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a graduate student understand an AI research paper. \"\n",
    "        \"You explain ideas clearly, with intuition and a simple example when useful.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    Here is OCR text from a page of a research paper. It may contain noise; ignore formatting\n",
    "    mistakes and focus on the main ideas.\n",
    "\n",
    "    ---- PAGE TEXT (CONTEXT) ----\n",
    "    {page_text}\n",
    "    ---- END PAGE TEXT ----\n",
    "\n",
    "    The user highlighted this specific passage on the page:\n",
    "\n",
    "    ---- HIGHLIGHTED PASSAGE ----\n",
    "    {highlighted_text}\n",
    "    ---- END HIGHLIGHTED PASSAGE ----\n",
    "\n",
    "    Task:\n",
    "    1. Explain what this highlighted passage is saying in simple terms.\n",
    "    2. Give the intuition behind it and why it matters in the context of the paper.\n",
    "    3. Provide a small concrete example if it helps.\n",
    "\n",
    "    Keep the explanation within 2–4 short paragraphs.\n",
    "    \"\"\")\n",
    "\n",
    "    answer = call_qwen(system_prompt, user_prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a5776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(image: Image.Image, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a page image and a natural language question,\n",
    "    run OCR then ask Qwen to answer using the page text.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload a page image first.\"\n",
    "\n",
    "    page_text = ocr_page_image(image)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a student understand an AI research paper. \"\n",
    "        \"You only use information that can reasonably be inferred from the OCR text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    Here is OCR text from a page of a research paper. It may be noisy; ignore formatting\n",
    "    mistakes and focus on the main ideas.\n",
    "\n",
    "    ---- OCR TEXT START ----\n",
    "    {page_text}\n",
    "    ---- OCR TEXT END ----\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\")\n",
    "\n",
    "    answer = call_qwen(system_prompt, user_prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "499aa89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_highlight_sections_from_image(image: Image.Image, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Given a page image, OCR it, pick top-k important paragraphs using a heuristic,\n",
    "    and then ask Qwen to justify why each one is important.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload a page image first.\"\n",
    "\n",
    "    page_text = ocr_page_image(image)\n",
    "    paragraphs = split_into_paragraphs(page_text)\n",
    "    if not paragraphs:\n",
    "        return \"Could not find enough text on this page to highlight.\"\n",
    "\n",
    "    scored = [(heuristic_score(p), p) for p in paragraphs]\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    top = scored[:top_k]\n",
    "\n",
    "    numbered_paras = \"\\n\\n\".join(\n",
    "        f\"[{i+1}] {p}\" for i, (_, p) in enumerate(top)\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a student quickly understand which parts \"\n",
    "        \"of a research paper page are most important.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    We OCR'd a page from a computer vision paper (e.g., EfficientViT).\n",
    "    Below are {top_k} candidate important snippets selected by a heuristic.\n",
    "\n",
    "    For each snippet:\n",
    "    - Explain in 1–2 sentences what it is talking about.\n",
    "    - Explain in 1–2 sentences why it is important for understanding the paper.\n",
    "    - Be concise.\n",
    "\n",
    "    ---- CANDIDATE 'PURPLE HIGHLIGHTS' ----\n",
    "    {numbered_paras}\n",
    "    ---- END SNIPPETS ----\n",
    "\n",
    "    Produce an answer in this format:\n",
    "\n",
    "    [1] <short explanation of content>\n",
    "        <why it is important>\n",
    "\n",
    "    [2] ...\n",
    "    \"\"\")\n",
    "\n",
    "    explanation = call_qwen(system_prompt, user_prompt)\n",
    "\n",
    "    display_text = \"Auto-selected important snippets (simulated purple highlights):\\n\\n\"\n",
    "    for i, (_, p) in enumerate(top):\n",
    "        display_text += f\"[{i+1}] {p}\\n\\n\"\n",
    "\n",
    "    display_text += \"Explanations:\\n\\n\" + explanation\n",
    "    return display_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18ca7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## CUA v0 – Ask Questions and Explain Highlights for a Paper Page\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(type=\"pil\", label=\"Paper page or table screenshot\")\n",
    "\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"Question about this page\",\n",
    "                    placeholder=\"e.g., 'What is the main idea of this page?'\",\n",
    "                )\n",
    "\n",
    "                highlight_input = gr.Textbox(\n",
    "                    label=\"Highlighted text (paste what you highlighted on the page)\",\n",
    "                    placeholder=\"Paste the yellow-highlighted sentence or paragraph here.\",\n",
    "                    lines=4,\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Answer Question\")\n",
    "                    explain_btn = gr.Button(\"Explain Highlighted Text\")\n",
    "\n",
    "                auto_btn = gr.Button(\"Auto-highlight important sections (purple)\")\n",
    "\n",
    "                table_btn = gr.Button(\"Explain table (upload/crop the table)\")\n",
    "\n",
    "                ablation_btn = gr.Button(\"Find most detrimental ablation\")\n",
    "\n",
    "            with gr.Column():\n",
    "                answer_output = gr.Textbox(\n",
    "                    label=\"Answer / Highlights / Table Explanation\",\n",
    "                    lines=20,\n",
    "                )\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=answer_question,\n",
    "            inputs=[image_input, question_input],\n",
    "            outputs=[answer_output],\n",
    "        )\n",
    "\n",
    "        explain_btn.click(\n",
    "            fn=explain_highlighted_text_from_image,\n",
    "            inputs=[image_input, highlight_input],\n",
    "            outputs=[answer_output],\n",
    "        )\n",
    "\n",
    "        auto_btn.click(\n",
    "            fn=auto_highlight_sections_from_image,\n",
    "            inputs=[image_input],\n",
    "            outputs=[answer_output],\n",
    "        )\n",
    "\n",
    "        table_btn.click(\n",
    "            fn=explain_table_from_image,\n",
    "            inputs=[image_input],\n",
    "            outputs=[answer_output],\n",
    "        )\n",
    "\n",
    "        ablation_btn.click(\n",
    "            fn=most_detrimental_ablation_from_image,\n",
    "            inputs=[image_input],\n",
    "            outputs=[answer_output],\n",
    "        )\n",
    "\n",
    "\n",
    "    return demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82e6a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7865\n",
      "* Running on public URL: https://f3cd21bead6361b760.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f3cd21bead6361b760.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = build_demo()\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7865,\n",
    "    share=True,\n",
    "    inbrowser=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
