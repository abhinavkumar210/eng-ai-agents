{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b2ee60",
   "metadata": {},
   "source": [
    "# Project — Part 4: Gradio App\n",
    "\n",
    "Abhinav Kumar\n",
    "12/11/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "try:\n",
    "    from pymongo import MongoClient\n",
    "    MONGO_ENABLED = True\n",
    "except ImportError:\n",
    "    MONGO_ENABLED = False\n",
    "\n",
    "FRAME_PATH = Path(\"/workspaces/eng-ai-agents/project/frames/latest.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_collection = None\n",
    "\n",
    "if MONGO_ENABLED:\n",
    "    try:\n",
    "        mongo_client = MongoClient(\"mongodb://localhost:27017\")\n",
    "        mongo_db = mongo_client[\"cua\"]\n",
    "        mongo_collection = mongo_db[\"interactions\"]\n",
    "        print(\"Mongo logging enabled.\")\n",
    "    except Exception as e:\n",
    "        print(\"Mongo not available, logging disabled:\", e)\n",
    "        mongo_collection = None\n",
    "else:\n",
    "    print(\"pymongo not installed, Mongo logging disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_frame() -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load the most recent frame saved by the WebRTC receiver.\n",
    "    \"\"\"\n",
    "    if not FRAME_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Frame not found at: {FRAME_PATH}\")\n",
    "    return Image.open(FRAME_PATH).convert(\"RGB\")\n",
    "\n",
    "# try:\n",
    "#     img = load_latest_frame()\n",
    "#     print(\"Loaded latest frame:\", img.size)\n",
    "# except Exception as e:\n",
    "#     print(\"Could not load latest frame yet:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import textwrap\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://host.docker.internal:11434\"\n",
    "MODEL_NAME = \"qwen2.5:latest\"\n",
    "\n",
    "def call_qwen(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call local Qwen via Ollama's /api/generate endpoint.\n",
    "    \"\"\"\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    System: {system_prompt}\n",
    "\n",
    "    User: {user_prompt}\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
    "    if not resp.ok:\n",
    "        print(\"Ollama error status:\", resp.status_code)\n",
    "        print(\"Ollama error body:\", resp.text[:1000])\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    return data.get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e57b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def ocr_image_to_text(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Run Tesseract OCR on a full paper page screenshot.\n",
    "\n",
    "    Input:  PIL.Image (RGB)\n",
    "    Output: extracted text as a string\n",
    "    \"\"\"\n",
    "\n",
    "    img = np.array(image)   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    scale = 1.5\n",
    "    img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * scale), int(img.shape[0] * scale)),\n",
    "        interpolation=cv2.INTER_CUBIC,\n",
    "    )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(\n",
    "        gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY\n",
    "    )\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, lang=\"eng\")\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_about_current_frame(question: str) -> str:\n",
    "    \"\"\"\n",
    "    High-level CUA function:\n",
    "      1. Load latest frame from disk.\n",
    "      2. Run OCR/VLM to get text.\n",
    "      3. Ask Qwen to answer the user's question based on that text.\n",
    "      4. Optionally log to MongoDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = load_latest_frame()\n",
    "    except Exception as e:\n",
    "        return f\"Error loading latest frame: {e}\"\n",
    "\n",
    "    try:\n",
    "        ocr_text = ocr_image_to_text(image)\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        return f\"Error during OCR/VLM processing:\\n{e}\\n\\n{tb}\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant reading a screenshot of an AI research paper. \"\n",
    "        \"You only see the OCR text I give you. Answer the user's question clearly and concisely. \"\n",
    "        \"If the answer is not visible in the text, say that you are not sure.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Here is the OCR text extracted from the current screen:\\n\\n\"\n",
    "        f\"{ocr_text}\\n\\n\"\n",
    "        f\"User question: {question}\\n\\n\"\n",
    "        \"Please answer in 2–5 sentences.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        answer = call_qwen(system_prompt, user_prompt)\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        return f\"Error calling Qwen LLM:\\n{e}\\n\\n{tb}\"\n",
    "\n",
    "    if mongo_collection is not None:\n",
    "        try:\n",
    "            mongo_collection.insert_one(\n",
    "                {\n",
    "                    \"timestamp\": datetime.utcnow(),\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"ocr_text\": ocr_text,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Mongo logging error:\", e)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_frame():\n",
    "    \"\"\"\n",
    "    For the Gradio 'Refresh' button: just reload latest.png.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = load_latest_frame()\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        # Gradio Image can also show None; we return None and let the text explain.\n",
    "        print(\"Error refreshing frame:\", e)\n",
    "        return None\n",
    "\n",
    "def gradio_answer(question: str):\n",
    "    \"\"\"\n",
    "    Thin wrapper around answer_question_about_current_frame for Gradio.\n",
    "    \"\"\"\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    return answer_question_about_current_frame(question)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Computer Using Agent — Demo\n",
    "\n",
    "        1. Make sure your WebRTC receiver is running: `make webrtc`  \n",
    "        2. Open the screen share page in your browser and start sharing the PDF window.  \n",
    "        3. Click **Refresh frame** to load the current screen.  \n",
    "        4. Ask a question about what is visible on the page.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        frame_img = gr.Image(label=\"Current Screen Frame\", type=\"pil\")\n",
    "        with gr.Column():\n",
    "            refresh_btn = gr.Button(\"Refresh frame from latest.png\")\n",
    "            question_box = gr.Textbox(\n",
    "                label=\"Question about the current page\",\n",
    "                placeholder=\"e.g., Explain the highlighted paragraph in simple terms.\",\n",
    "                lines=3,\n",
    "            )\n",
    "            answer_box = gr.Markdown(label=\"Answer\")\n",
    "\n",
    "    refresh_btn.click(fn=refresh_frame, outputs=frame_img)\n",
    "    question_box.submit(fn=gradio_answer, inputs=question_box, outputs=answer_box)\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "    ask_btn.click(fn=gradio_answer, inputs=question_box, outputs=answer_box)\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7861,\n",
    "    share=False,\n",
    "    inbrowser=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
