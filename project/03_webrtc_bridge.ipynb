{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd1d50f",
   "metadata": {},
   "source": [
    "# Project — Part 3: WebRTC\n",
    "\n",
    "Abhinav Kumar\n",
    "12/7/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b09a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "FRAME_PATH = Path(\"/workspaces/eng-ai-agents/project/frames/latest.png\")\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://host.docker.internal:11434\"\n",
    "MODEL_NAME = \"qwen2.5:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42be106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_qwen(system_prompt: str, user_prompt: str) -> str:\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    System: {system_prompt}\n",
    "\n",
    "    User: {user_prompt}\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
    "    if not resp.ok:\n",
    "        print(\"Ollama error status:\", resp.status_code)\n",
    "        print(\"Ollama error body:\", resp.text[:500])\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    return data.get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6fd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_page_from_file(path: Path) -> str:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No frame found at {path}\")\n",
    "\n",
    "    pil_image = Image.open(path).convert(\"RGB\")\n",
    "    img = np.array(pil_image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    scale = 1.5\n",
    "    img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * scale), int(img.shape[0] * scale)),\n",
    "        interpolation=cv2.INTER_CUBIC,\n",
    "    )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, lang=\"eng\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d398c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_about_current_frame(question: str) -> str:\n",
    "    if not FRAME_PATH.exists():\n",
    "        return f\"Frame file not found at {FRAME_PATH}. Is the WebRTC receiver saving frames?\"\n",
    "\n",
    "    page_text = ocr_page_from_file(FRAME_PATH)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a student understand whatever is on the screen. \"\n",
    "        \"You only use information that can reasonably be inferred from the OCR text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "    Here is OCR text from a screenshot of the screen (likely a paper page):\n",
    "\n",
    "    ---- OCR TEXT START ----\n",
    "    {page_text}\n",
    "    ---- OCR TEXT END ----\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\")\n",
    "\n",
    "    return call_qwen(system_prompt, user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32568f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper introduces EfficientViT, a new vision model designed for efficient high-resolution dense prediction tasks. It uses multi-scale linear attention to achieve global receptive fields and multi-scale learning with lightweight operations, addressing the hardware inefficiency found in previous models like SegFormer and SegNeXt. EfficientViT demonstrates significant performance gains and speedups on various hardware platforms while maintaining or improving accuracy in tasks such as semantic segmentation, super-resolution, and instance segmentation.\n"
     ]
    }
   ],
   "source": [
    "ans = answer_question_about_current_frame(\n",
    "    \"Summarize this screen for me in 3–4 sentences.\"\n",
    ")\n",
    "print(ans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
