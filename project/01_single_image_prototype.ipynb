{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d657cb",
   "metadata": {},
   "source": [
    "# Project — Part 1: Single Image\n",
    "\n",
    "Abhinav Kumar\n",
    "12/7/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_PATH = \"/workspaces/eng-ai-agents/project/data/efficientvit_page1.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f166ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2205.14756v6 [cs.CV] 6 Feb 2024\n",
      "\n",
      "Efficient ViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n",
      "\n",
      "Han Cai!, Junyan Li?, Muyan Hu’, Chuang Gant, Song Han!\n",
      "IMIT. ?Zhejiang University, “Tsinghua University, MIT-IBM Watson Al Lab\n",
      "\n",
      "hetps:/ (github\n",
      "\n",
      "Abstract\n",
      "\n",
      "High-resolwion dense prediction enables many appeal\n",
      "ing real-world applications, such as computational pho-\n",
      "lugraphy, autonomous driving. ete. However the vast\n",
      "computational cost makes deploying state-of-the-art high-\n",
      "resolution dense prediction models on hundware devices dif-\n",
      "ficult. This work presents EfficientVIT, a new family of high-\n",
      "resolution vision models with novel mulsi-scale linear atten-\n",
      "Jion, Untike prioe high-resolution dease prediction models\n",
      "that rely on heavy softmax attention, handware-inefficient\n",
      "large-temel convolution, or complicated topology struc\n",
      "lure to obtain good performances, our multi-scale linear\n",
      "attention achieves the global receptive field and multi-seate\n",
      "leaming (two desirable feutures Jor high-resolution dense\n",
      "prediction) with anly lightweight and hardware-efficient op-\n",
      "erations. As such, EfficlentViT delivers remarkable per\n",
      "formance guins over previous state-of-the-art models with\n",
      "‘significant speedup on diverse hardware platforms, includ-\n",
      "ing mobile CPU, edge GPU, and cloud GPU. Without\n",
      "performance toss on Cityscapes, our Efficiem WF provides\n",
      "up tw 13.9% and 6.2% GPU latency reduction over Seg-\n",
      "Former and SegneXt, respectively. For super-resotutton,\n",
      "EfficiemViT delivers\n"
     ]
    }
   ],
   "source": [
    "def ocr_page(image_path: str) -> str:\n",
    "    \"\"\"Run Tesseract OCR on a full paper page screenshot.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    scale = 1.5\n",
    "    img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * scale), int(img.shape[0] * scale)),\n",
    "        interpolation=cv2.INTER_CUBIC,\n",
    "    )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, lang=\"eng\")\n",
    "    return text\n",
    "\n",
    "page_text = ocr_page(IMAGE_PATH)\n",
    "print(page_text[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8be4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import textwrap\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://host.docker.internal:11434\"\n",
    "MODEL_NAME = \"qwen2.5:latest\"\n",
    "\n",
    "def call_qwen(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call local Qwen via Ollama's /api/generate endpoint.\n",
    "    \"\"\"\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    System: {system_prompt}\n",
    "\n",
    "    User: {user_prompt}\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
    "    if not resp.ok:\n",
    "        print(\"Ollama error status:\", resp.status_code)\n",
    "        print(\"Ollama error body:\", resp.text[:1000])\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    return data.get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bceb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen test: 'OK'\n"
     ]
    }
   ],
   "source": [
    "test_answer = call_qwen(\n",
    "    \"You are a concise assistant.\",\n",
    "    \"Reply with the single word OK.\"\n",
    ")\n",
    "print(\"Qwen test:\", repr(test_answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf927439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces EfficientViT, a novel vision model designed for high-resolution dense prediction. It uses multi-scale linear attention to achieve both global receptive fields and multi-scale learning with lightweight operations, addressing hardware efficiency. This approach outperforms existing models like SegFormer and SegNeXt on various tasks, including semantic segmentation and super-resolution, while providing significant speedups. The core innovation lies in replacing inefficient softmax attention with ReLU linear attention, which maintains performance gains without sacrificing speed on different hardware platforms.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are an AI tutor explaining computer vision research papers \"\n",
    "    \"to a graduate student. Use simple, precise language.\"\n",
    ")\n",
    "\n",
    "user_question = \"Summarize the main idea of this page in 4–5 sentences.\"\n",
    "\n",
    "user_prompt = textwrap.dedent(f\"\"\"\n",
    "Here is OCR text from the first page of an AI paper (EfficientViT).\n",
    "The text may contain noise from OCR; ignore formatting issues and focus\n",
    "on the key ideas.\n",
    "\n",
    "---- OCR TEXT START ----\n",
    "{page_text}\n",
    "---- OCR TEXT END ----\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\")\n",
    "\n",
    "summary_answer = call_qwen(system_prompt, user_prompt)\n",
    "print(summary_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f73c2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Explanation of the Highlighted Passage\n",
      "\n",
      "The highlighted passage states that high-resolution dense prediction (HRDP) is crucial for numerous real-world applications, including computational photography and autonomous driving. This means that being able to predict or analyze images at a very fine level—i.e., with a lot of detail—is essential for these practical uses.\n",
      "\n",
      "### Simple Restatement and Intuition\n",
      "\n",
      "In simpler terms, high-resolution dense prediction allows systems to understand detailed visual information accurately, which is vital for technologies like computational photography (where every pixel counts) and autonomous driving (where precise scene understanding can save lives). The intuition here lies in the fact that higher resolution images provide more granular data, enabling better decision-making processes. For instance, in autonomous driving, a high-resolution image might help detect pedestrians or obstacles from further distances, enhancing safety.\n",
      "\n",
      "### Concrete Example\n",
      "\n",
      "Imagine you are developing an app for computational photography. When you take a picture of a landscape with a lot of details (trees, rocks, water), you want the photo to capture every single element accurately. High-resolution dense prediction helps achieve this by processing each pixel precisely, ensuring that the software can recognize and enhance even minute details like the texture of leaves or ripples in water. Similarly, for autonomous driving, if your vehicle's camera captures a high-resolution image, it can identify objects more accurately at greater distances, improving the car’s reaction time to avoid hazards.\n",
      "\n",
      "This capability is significant because it directly impacts the quality and reliability of various applications that depend on visual data, making them more effective and safer.\n"
     ]
    }
   ],
   "source": [
    "def explain_highlighted_text(page_text: str, highlighted_text: str) -> str:\n",
    "    \"\"\"Tutorial-style explanation of a highlighted snippet.\"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an AI tutor helping a graduate student understand an AI \"\n",
    "        \"research paper. Explain things clearly with intuition and a simple example.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = textwrap.dedent(f\"\"\"\n",
    "Here is noisy OCR text from a page of a research paper:\n",
    "\n",
    "---- PAGE TEXT ----\n",
    "{page_text}\n",
    "---- END PAGE TEXT ----\n",
    "\n",
    "The user highlighted this passage:\n",
    "\n",
    "---- HIGHLIGHTED PASSAGE ----\n",
    "{highlighted_text}\n",
    "---- END HIGHLIGHTED PASSAGE ----\n",
    "\n",
    "Explain what this highlighted passage means.\n",
    "1. Restate it in simple terms.\n",
    "2. Give the intuition and why it matters.\n",
    "3. Give a small concrete example if it helps.\n",
    "\n",
    "Keep the answer within 2–4 short paragraphs.\n",
    "\"\"\")\n",
    "\n",
    "    return call_qwen(system_prompt, user_prompt)\n",
    "\n",
    "fake_highlight = \"High-resolution dense prediction enables many real-world applications such as computational photography and autonomous driving.\"\n",
    "\n",
    "highlight_explanation = explain_highlighted_text(page_text, fake_highlight)\n",
    "print(highlight_explanation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
